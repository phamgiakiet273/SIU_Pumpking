# file: event_localizer.py

import json
import logging
import re
from pathlib import Path
from typing import List, Optional, Dict, Any

import numpy as np
import torch
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration

# ==============================================================================
# HELPER FUNCTION
# ==============================================================================


def sample_frames_evenly(frame_paths: List[str], max_frames: int = 40) -> List[str]:
    """
    Ch·ªçn m·∫´u c√°c frame m·ªôt c√°ch "ƒë·ªÅu" v√† c√≥ √Ω nghƒ©a t·ª´ danh s√°ch.
    """
    if not frame_paths:
        return []

    unique_paths = sorted(list(set(frame_paths)), key=frame_paths.index)
    n = len(unique_paths)

    if n <= max_frames:
        return unique_paths

    indices_to_keep = {0, n - 1}
    num_middle_frames = max_frames - 2

    if num_middle_frames > 0:
        middle_indices = np.linspace(1, n - 2, num=num_middle_frames, dtype=int)
        indices_to_keep.update(middle_indices)

    sorted_indices = sorted(list(indices_to_keep))
    return [unique_paths[i] for i in sorted_indices]


# ==============================================================================
# MAIN CLASS
# ==============================================================================


class EventLocalizer:
    """
    S·ª≠ d·ª•ng model Qwen2.5-VL ƒë·ªÉ x√°c ƒë·ªãnh v√† khoanh v√πng c√°c s·ª± ki·ªán trong m·ªôt
    ph√¢n c·∫£nh video (shot), d·ª±a tr√™n h√¨nh ·∫£nh, √¢m thanh (S2T) v√† ng·ªØ c·∫£nh vƒÉn b·∫£n.
    """

    def __init__(self, model_path: str):
        """
        Kh·ªüi t·∫°o EventLocalizer b·∫±ng c√°ch t·∫£i model v√† processor.
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logging.info(f"üöÄ [EventLocalizer] Kh·ªüi t·∫°o model tr√™n device: {self.device}")

        try:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float16,  # torch_dtype=torch.float16,
                device_map="auto",
            )
            self.processor = AutoProcessor.from_pretrained(model_path)
            logging.info("‚úÖ [EventLocalizer] Model v√† processor ƒë√£ s·∫µn s√†ng.")
        except Exception as e:
            logging.critical(
                f"‚ùå [EventLocalizer] L·ªói nghi√™m tr·ªçng khi t·∫£i model: {e}", exc_info=True
            )
            raise

    def _load_images(self, frame_paths: List[str]) -> Optional[List[np.ndarray]]:
        """T·∫£i ·∫£nh t·ª´ danh s√°ch ƒë∆∞·ªùng d·∫´n v√† chuy·ªÉn th√†nh list c√°c numpy array."""
        images = []

        for path_str in frame_paths:
            p = Path(path_str).with_suffix(".jpg")
            if not p.exists():
                logging.warning(f"  [EventLocalizer] B·ªè qua frame kh√¥ng t·ªìn t·∫°i: {p}")
                continue
            try:
                with Image.open(p) as img:
                    images.append(np.array(img.convert("RGB")))
            except Exception as e:
                logging.warning(f"  [EventLocalizer] Kh√¥ng th·ªÉ m·ªü ·∫£nh {p}: {e}")

        return images if images else None

    def _build_prompt(self, s2t: str, context_text: str) -> str:
        """T·∫°o prompt chi ti·∫øt cho model."""
        return f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch video. Nhi·ªám v·ª• c·ªßa b·∫°n l√† x√°c ƒë·ªãnh v√† ph√¢n ƒëo·∫°n c√°c s·ª± ki·ªán ri√™ng bi·ªát trong m·ªôt video clip.
                **D·ªØ li·ªáu cung c·∫•p:**
                1. **Keyframes:** M·ªôt chu·ªói c√°c khung h√¨nh ƒë·∫°i di·ªán cho video.
                2. **S2T (Speech-to-Text):** VƒÉn b·∫£n ƒë∆∞·ª£c nh·∫≠n d·∫°ng t·ª´ gi·ªçng n√≥i trong video.
                    <s2t>
                    {s2t if s2t else "Kh√¥ng c√≥ gi·ªçng n√≥i."}
                    </s2t>
                3. **Ng·ªØ c·∫£nh:** Th√¥ng tin b·ªï sung v·ªÅ s·ª± ki·ªán ch√≠nh c√≥ trong video.
                    <context>
                    {context_text if context_text else "Kh√¥ng c√≥ ng·ªØ c·∫£nh b·ªï sung, t·∫≠p trung v√†o c√°c khug h√¨nh trong video"}
                    </context>

                **Y√™u c·∫ßu:**
                1. **Ph√¢n t√≠ch K·ªπ l∆∞·ª°ng:** K·∫øt h·ª£p th√¥ng tin t·ª´ H√åNH ·∫¢NH, S2T, v√† NG·ªÆ C·∫¢NH ƒë·ªÉ ƒë∆∞a ra k·∫øt qu·∫£ ch√≠nh x√°c nh·∫•t.
                2. **Ph√¢n ƒëo·∫°n S·ª± ki·ªán:** Chia video th√†nh c√°c s·ª± ki·ªán ri√™ng bi·ªát. V·ªõi m·ªói s·ª± ki·ªán, cung c·∫•p:
                    - "start": Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu s·ª± ki·ªán (ƒë·ªãnh d·∫°ng "mm:ss.ff").
                    - "end": Th·ªùi ƒëi·ªÉm k·∫øt th√∫c s·ª± ki·ªán (ƒë·ªãnh d·∫°ng "mm:ss.ff").
                    - "description": M√¥ t·∫£ h√†nh ƒë·ªông ch√≠nh c·ªßa s·ª± ki·ªán. **Quan tr·ªçng: Trong m√¥ t·∫£ n√†y, h√£y tr√≠ch xu·∫•t v√† ghi l·∫°i TO√ÄN B·ªò vƒÉn b·∫£n (OCR) m√† b·∫°n nh√¨n th·∫•y** trong c√°c khung h√¨nh thu·ªôc s·ª± ki·ªán ƒë√≥. Bao g·ªìm ti√™u ƒë·ªÅ, t√™n ng∆∞·ªùi, d√≤ng ch·ªØ ch·∫°y, v√† b·∫•t k·ª≥ k√Ω t·ª± n√†o kh√°c ƒë·ªÉ di·ªÖn ƒë·∫°t ng·ªØ c·∫£nh.
                3. **Tr√°nh Suy di·ªÖn:** Tuy·ªát ƒë·ªëi kh√¥ng t·ª± √Ω th√™m th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.
                4. **ƒê·ªãnh d·∫°ng Output:** Ch·ªâ tr·∫£ v·ªÅ m·ªôt m·∫£ng JSON h·ª£p l·ªá. KH√îNG th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch hay vƒÉn b·∫£n n√†o kh√°c ngo√†i kh·ªëi JSON.
                """

    def _parse_json_from_model_output(self, output_text: str) -> Optional[List[Dict]]:
        """
        Tr√≠ch xu·∫•t v√† parse kh·ªëi JSON t·ª´ output th√¥ c·ªßa model m·ªôt c√°ch an to√†n.
        """
        json_pattern = re.search(
            r"```json\s*([\s\S]*?)\s*```|(\[[\s\S]*\]|{[\s\S]*})", output_text
        )
        if not json_pattern:
            logging.warning(
                "  [EventLocalizer] Kh√¥ng t√¨m th·∫•y kh·ªëi JSON trong output c·ªßa model."
            )
            return None

        json_str = json_pattern.group(1) or json_pattern.group(2)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            logging.error(
                f"  [EventLocalizer] L·ªói khi parse JSON t·ª´ chu·ªói: {json_str[:200]}..."
            )
            return None

    def localize_events_in_shot(
        self, shot_data: Dict[str, Any], context_text: str, max_frames: int = 45
    ) -> Optional[Dict[str, Any]]:
        """
        Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ x·ª≠ l√Ω m·ªôt shot duy nh·∫•t.
        """
        try:
            frames = shot_data.get("frames", [])
            if not frames:
                logging.warning(
                    "  [EventLocalizer] Shot kh√¥ng c√≥ danh s√°ch frames, b·ªè qua."
                )
                return None

            selected_frames = sample_frames_evenly(frames, max_frames)
            images = self._load_images(selected_frames)
            if not images:
                logging.error(
                    "  [EventLocalizer] Kh√¥ng th·ªÉ t·∫£i b·∫•t k·ª≥ frame n√†o, kh√¥ng th·ªÉ x·ª≠ l√Ω."
                )
                return None

            video_data = np.stack(images)
            s2t = shot_data.get("s2t", "")
            prompt = self._build_prompt(s2t=s2t, context_text=context_text)

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "video": video_data},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            inputs = self.processor(
                text=[text_prompt],
                videos=[video_data],
                padding=True,
                return_tensors="pt",
            ).to(self.device)

            logging.info(
                f"  [EventLocalizer] ƒêang x·ª≠ l√Ω {len(images)} frame tr√™n model..."
            )
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs, max_new_tokens=32768, do_sample=False
                )

            output_ids = generated_ids[0][len(inputs.input_ids[0]) :]
            raw_output_text = self.processor.decode(
                output_ids, skip_special_tokens=True
            ).strip()

            parsed_json = self._parse_json_from_model_output(raw_output_text)

            return {
                "localized_events": parsed_json,
                "model_raw_output": raw_output_text,
                "context_provided": context_text,
            }

        except Exception as e:
            logging.error(
                f"‚ùå [EventLocalizer] G·∫∑p l·ªói kh√¥ng mong mu·ªën khi x·ª≠ l√Ω shot: {e}",
                exc_info=True,
            )
            return None
