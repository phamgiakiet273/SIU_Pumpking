# file: title_extractor.py
import logging
import torch
import numpy as np
from PIL import Image
from concurrent.futures import ThreadPoolExecutor
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig,
)


class TitleExtractor:
    """
    Má»™t lá»›p Ä‘á»ƒ táº£i mÃ´ hÃ¬nh Qwen2.5-VL vÃ  trÃ­ch xuáº¥t tiÃªu Ä‘á» tá»« cÃ¡c frame áº£nh.
    """

    def __init__(self, model_path: str):
        logging.info("ğŸš€ [TitleExtractor] Äang khá»Ÿi táº¡o...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.processor = self._load_model(model_path)
        logging.info("âœ… [TitleExtractor] Äá»‘i tÆ°á»£ng Ä‘Ã£ sáºµn sÃ ng.")

    def _load_model(self, model_path: str):
        logging.info(f"ğŸ§  [TitleExtractor] Äang táº£i mÃ´ hÃ¬nh tá»«: {model_path}")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        processor = AutoProcessor.from_pretrained(model_path)
        return model, processor

    def _load_frames_parallel(self, frame_paths: list, max_workers: int = 16):
        def _load_single_frame(path: str):
            try:
                return np.array(Image.open(f"{path}.jpg").convert("RGB"))
            except Exception:
                return None

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = executor.map(_load_single_frame, frame_paths)
            loaded_frames = [frame for frame in results if frame is not None]

        return np.stack(loaded_frames) if loaded_frames else None

    def extract_title(self, frame_paths: list) -> str:
        if not frame_paths:
            return "Lá»—i: Danh sÃ¡ch frame rá»—ng."

        logging.info(f"ğŸ–¼ï¸  [TitleExtractor] Äang táº£i {len(frame_paths)} khung hÃ¬nh...")
        loaded_frames = self._load_frames_parallel(frame_paths)
        if loaded_frames is None:
            return "Lá»—i: KhÃ´ng thá»ƒ táº£i Ä‘Æ°á»£c frames."

        user_prompt = "CÃ¡c khung hÃ¬nh nÃ y Ä‘Æ°á»£c trÃ­ch tá»« pháº§n Ä‘áº§u cá»§a má»™t báº£n tin thá»i sá»±. HÃ£y quÃ©t ká»¹ cÃ¡c khung hÃ¬nh Ä‘á»ƒ tÃ¬m vÃ  trÃ­ch xuáº¥t chÃ­nh xÃ¡c vÄƒn báº£n cá»§a tiÃªu Ä‘á» chÃ­nh. Chá»‰ tráº£ vá» ná»™i dung vÄƒn báº£n cá»§a tiÃªu Ä‘á»."
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": loaded_frames},
                    {"type": "text", "text": user_prompt},
                ],
            }
        ]
        text_prompt = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = self.processor(
            text=[text_prompt],
            videos=[loaded_frames],
            padding=True,
            return_tensors="pt",
        ).to(self.model.device)

        logging.info("ğŸ§  [TitleExtractor] Äang suy luáº­n...")
        generated_ids = self.model.generate(**inputs, max_new_tokens=120)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :]
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False,
        )[0]

        return output_text.strip()
