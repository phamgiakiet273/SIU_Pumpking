# file: qwen25vl.py

import os
import dotenv
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from typing import List, Dict, Any
from utils.logger import get_logger

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng v√† thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
dotenv.load_dotenv()
# os.environ["CUDA_VISIBLE_DEVICES"] = os.getenv("QWEN25_CUDA_VISIBLE_DEVICES")
Model_path = os.getenv("MODEL_QWEN25_WEIGHT_FOLDER")
logger = get_logger()


class Qwen25VL:
    """
    L·ªõp l√µi ƒë·ªÉ t·∫£i v√† ch·∫°y suy lu·∫≠n v·ªõi m√¥ h√¨nh Qwen2.5-VL.
    L·ªõp n√†y kh√¥ng bi·∫øt v·ªÅ logic nghi·ªáp v·ª• c·ª• th·ªÉ, ch·ªâ t·∫≠p trung v√†o m√¥ h√¨nh.
    """

    def __init__(self, model_path: str = None):
        if model_path is None:
            model_path = Model_path
        if not model_path:
            raise ValueError("MODEL_QWEN25_WEIGHT_FOLDER ch∆∞a ƒë∆∞·ª£c set trong .env")

        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(
            f"üöÄ [Qwen25VL] Kh·ªüi t·∫°o m√¥ h√¨nh '{self.model_path}' tr√™n {self.device}..."
        )

        try:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                self.model_path, torch_dtype="auto", device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(self.model_path)
            logger.info("‚úÖ [Qwen25VL] M√¥ h√¨nh v√† b·ªô x·ª≠ l√Ω ƒë√£ s·∫µn s√†ng.")
        except Exception as e:
            logger.critical(
                f"‚ùå [Qwen25VL] L·ªói nghi√™m tr·ªçng khi t·∫£i m√¥ h√¨nh: {e}", exc_info=True
            )
            raise

    def infer(self, messages: List[Dict[str, Any]], max_new_tokens: int = 1024) -> str:
        """
        Th·ª±c hi·ªán suy lu·∫≠n chung tr√™n m√¥ h√¨nh v·ªõi m·ªôt payload tin nh·∫Øn.
        """
        try:
            # Chu·∫©n b·ªã prompt v√† video/·∫£nh t·ª´ `messages`
            text_prompt = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            videos = [
                content_item["video"]
                for message in messages
                if message["role"] == "user"
                for content_item in message["content"]
                if content_item["type"] == "video"
            ]

            inputs = self.processor(
                text=[text_prompt], videos=videos, padding=True, return_tensors="pt"
            ).to(self.device)

            # Th·ª±c hi·ªán suy lu·∫≠n
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                )

            # Gi·∫£i m√£ k·∫øt qu·∫£
            output_ids = generated_ids[0][len(inputs["input_ids"][0]) :]
            response = self.processor.decode(
                output_ids, skip_special_tokens=True
            ).strip()
            return response

        except Exception as e:
            logger.error(
                f"‚ùå [Qwen25VL] Qu√° tr√¨nh suy lu·∫≠n th·∫•t b·∫°i: {e}", exc_info=True
            )
            return "L·ªói: Qu√° tr√¨nh suy lu·∫≠n th·∫•t b·∫°i."
